---
title: "Practical Machine Learning Course Project"
author: "Jelena Erdmann"
date: "4/18/2020"
output: html_document
---

```{r libraries, echo = FALSE, warning = FALSE, message = FALSE}
library(tidyr)
library(ggplot2)
library(dplyr)
library(caret)
```


```{r data, echo = FALSE, warning = FALSE, message = FALSE}
testing <- read.csv("pml-testing.csv")
training <- read.csv("pml-training.csv")
```

# About the Dataset

[groupware - Weight Lifting Exercise Dataset](http://groupware.les.inf.puc-rio.br/har)
Extract:
Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).


## Goal: predict the manner in which the participants did the exercise

# 1 Split Training Set to obtain Test and Train Set 
```{r var, message = FALSE, warning = FALSE}
set.seed(509)
inTrain  <- createDataPartition(training$classe, p=0.7, list=FALSE)
TrainSet <- training[inTrain, ]
TestSet  <- training[-inTrain, ]
```


# 2 Evaluate Variables
```{r, message = FALSE, warning = FALSE}
table(sapply(training, class))
#### Remove User and time specific data
training_filt <- TrainSet[, -c(1,2)]
training_filt <- training_filt[, !grepl("time", names(training_filt))]
### create vector that finds factor variables with > 20 levels -> better numeric
vec_fact <- sapply(training_filt, class) == "factor" & sapply(sapply(training_filt, levels), length) > 20
training_filt[,vec_fact] <- lapply(training_filt[,vec_fact], function(x) as.numeric(as.character(x)))
table(sapply(training_filt, class))
### have a look at the remaining factor variables
#summary(training_filt[,sapply(training_filt, class) == "factor"])
### no information left (except the outcome variable), remove variables
vec_rm <- sapply(training_filt, class) != "factor"
vec_rm[length(vec_rm)] <- TRUE  ## classes should stay a factor variable
training_filt <- training_filt[,vec_rm]
table(sapply(training_filt, class))
### have a look at the integer variables
vec_int <- sapply(training_filt, class) == "integer"
#summary(training_filt[,vec_int])
### Some variables have too many NA values, will be removed together with numeric...
### too many missing values to justify  imputation
table(unlist(lapply(training_filt, function(x) sum(is.na(x)))))
vec_too_many_na <- unlist(lapply(training_filt, function(x) sum(is.na(x))) < (nrow(training_filt)/10))
table(vec_too_many_na)
### 54 Variables remain
training_filt <- training_filt[,vec_too_many_na]
### have a look at the numeric variables, are there some with near zero variability left? 
nzv <- nearZeroVar(training_filt, saveMetrics = TRUE)
table(nzv$nzv)
```

After filtering the variables there are 54 resonable variables left. Next I had a detailled look at the variables. From the plots two outliers were observable. These will be removed in the next step.
```{r plot, message = FALSE, warning = FALSE}
### Have a look at the remaining variables
n1 <- table(training_filt$classe)["A"]
n2 <- n1 + table(training_filt$classe)["B"]
n3 <- n2 + table(training_filt$classe)["C"]
n4 <- n3 + table(training_filt$classe)["D"]
i = 32 # One Example plot with outlier
#for(i in 1:length(training_filt)){  #length(training_filt)
      print(ggplot(training_filt, aes(x = 1:nrow(training_filt), y = training_filt[,i],colour = training[inTrain, ]$user_name))+
            geom_point()+
            geom_vline(xintercept = c(n1, n2, n3, n4))+
            ggtitle(paste(i, names(training_filt)[i])))
#}
### remove 2 outliers
vec_out1 <- abs(training_filt$gyros_dumbbell_x) < 50
training_filt <- training_filt[vec_out1,]
vec_out2 <- abs(training_filt$magnet_dumbbell_y) < 1000
training_filt <- training_filt[vec_out2,]
```

# 3 Find best Model & use cross validation
I tried 4 different prediction Models. Out of these "random forest" and boosting based on trees" were found to be best suited for prediction. All models included 3 times cross-validation.
```{r, message = FALSE, warnings = FALSE}
### trees
set.seed(77)
controlRF <- trainControl(method="cv", number=3, verboseIter=FALSE)
model_rpart <- train(classe ~ ., data = training_filt, 
               method = "rpart", trControl = controlRF) 
pred_rpart <- predict(model_rpart, newdata = TestSet)
confusionMatrix(pred_rpart, TestSet$classe)
### linear discriminant analysis
set.seed(88)
controlRF <- trainControl(method="cv", number=3, verboseIter=FALSE)
model_lda <- train(classe ~ ., data = training_filt, 
               method = "lda", trControl = controlRF) 
pred_lda <- predict(model_lda, newdata = TestSet)
confusionMatrix(pred_lda, TestSet$classe)
### boosting with trees
set.seed(99)
controlRF <- trainControl(method="cv", number=3, verboseIter=FALSE)
model_gbm <- train(classe ~ ., data = training_filt, 
               method = "gbm", trControl = controlRF, verbose = FALSE) 
pred_gbm <- predict(model_gbm, newdata = TestSet)
confusionMatrix(pred_gbm, TestSet$classe)
#### random forest
set.seed(66)
controlRF <- trainControl(method="cv", number=3, verboseIter=FALSE)
model_rf <- train(classe ~ ., data = training_filt, 
               method = "rf", trControl = controlRF) 
model_rf$finalModel
pred_rf <- predict(model_rf, newdata = TestSet)
confusionMatrix(pred_rf, TestSet$classe)
```

### Predict Test Cases with Random Forest Model
The Random Forest Model has the best Accuracy with 0.9973 and will be used to predict the 20 test cases.
```{r}
predict(model_rf, testing)
```