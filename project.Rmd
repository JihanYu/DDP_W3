---
title: ""
output: html_document
---

## 1. Title : 

## 2. Synopsis

## 3. Data analysis
```{r setup, echo=FALSE, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, fig.align = "center", message=FALSE, warning=FALSE, 
                      fig.height=3, fig.width=3, cache=TRUE, dpi = 300)
```

### Loading & preprocessing data
```{r loading & preprocessing data}
pml.training <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", header=TRUE)
pml.testing <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", header=TRUE)

library(caret);  library(ggplot2);  library(rattle)
training <- pml.training[, c(8:160)]    # Fitbit data only

# convert variable type : factor to numeric 
factor.id <- which(sapply(training, is.factor));  factor.n <- length(factor.id)
for(i in factor.id[-factor.n]){
  training[, i] <- as.character(training[, i])
  training[, i] <- as.numeric(training[, i])
}

##### Imputation #####
# apply all NA values to zero (0)
# training[is.na(training)] <- 0

# select variables which have NA less than half
n.na <- function(vec){
	return ( sum(is.na(vec), na.rm=TRUE) )
}
n.row <- nrow(training)
na.id <- which( sapply(training, n.na) > n.row/2 )
training <- training[, -na.id]
dim(training)

##### Spliting the training data to training subset and testing subset #####
inTrain <- createDataPartition(training$classe, p=0.7, list=FALSE)
training.in <- training[inTrain,];  testing.in <- training[-inTrain,]
dim(training.in);  dim(testing.in)
```

### Tree model 
```{r Training - tree model}
##### Tree model #####
modFit.rpart <- train(classe ~ ., method="rpart", data=training.in)
print(modFit.rpart)
fancyRpartPlot(modFit.rpart$finalModel)

pr.train.rpart <- predict(modFit.rpart, newdata=training.in)   # Training subset
confusionMatrix(pr.train.rpart, training.in$classe)

pr.test.rpart <- predict(modFit.rpart, newdata=testing.in)     # Testing subset
confusionMatrix(pr.test.rpart, testing.in$classe)
```

### Prediction the classe of the pml.testing data
```{r predicting the pml.testing data}
##### Apply the tree model to pml.testing data #####
testing <- pml.testing[, c(8:160)]
factor.id <- which(sapply(testing, is.factor));  factor.n <- length(factor.id)

for(i in factor.id[-factor.n]){
  testing[, i] <- as.character(testing[, i])
  testing[, i] <- as.numeric(testing[, i])
}

testing[is.na(testing)] <- 0
predict(modFit.rpart, newdata=testing)
```

### Unsupervised model - kmeans
```{r modeling - unsupervised kmeans}
kMeans1 <- kmeans(subset(training.in, select=-c(classe)), centers=5)

res.kmeans <- kMeans1$cluster
res.kmeans[res.kmeans==1] <- "A";  res.kmeans[res.kmeans==2] <- "B";  
res.kmeans[res.kmeans==3] <- "C";  res.kmeans[res.kmeans==4] <- "D";  
res.kmeans[res.kmeans==5] <- "E"
res.kmeans <- as.factor(res.kmeans)
training.in$clusters <- res.kmeans

confusionMatrix(training.in$clusters, training.in$classe)

modFit.kmeans <- train(clusters ~ ., data=subset(training.in, select=-c(classe)), method="rpart")
pr.train.kmeans <- predict(modFit.kmeans, newdata=training.in)
confusionMatrix(pr.train.kmeans, training.in$classe)

pr.test.kmeans <- predict(modFit.kmeans, newdata=testing.in)
confusionMatrix(pr.test.kmeans, testing.in$classe)

predict(modFit.kmeans, newdata=testing)
```
